
save_data: data_run/huggingface/run/example
## Where the vocab(s) will be written
src_vocab: data_run/huggingface/run/converted.vocab.src
# n_sample: -1
src_vocab_size: 60000
tgt_vocab_size: 60000
src_subword_type: bpe
src_subword_model: /nas-labs/LM/valentin_work/OpenNMT-py/data_run/huggingface/merges.txt
src_onmttok_kwargs: '{"mode": "aggressive",  "preserve_placeholders":
  True, "soft_case_regions": True, "preserve_segmented_tokens":
  True}'
transforms: [onmt_tokenize, filtertoolong]
src_seq_length: 512
tgt_seq_length: 512

# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
    corpus:
        path_src: /nas-labs/LM/valentin_work/OpenNMT-py/data_run/wikitext-103-raw/wiki.train.raw.filtered
        weight: 1
    valid:
        path_src: /nas-labs/LM/valentin_work/OpenNMT-py/data_run/wikitext-103-raw/wiki.valid.raw

# Train on multi GPU
world_size: 1
gpu_ranks: [0]

# Where to save the checkpoints
save_model: /nas-labs/LM/valentin_work/OpenNMT-py/data_run/huggingface/run/model-lm-medium
save_checkpoint_steps: 1
train_steps: 2
valid_steps: 2
report_every: 2

# Model
model_task: lm
encoder_type: transformer_lm
decoder_type: transformer_lm
position_encoding: true
dec_layers: 24
heads: 16
rnn_size: 1024
word_vec_size: 1024
transformer_ff: 4096
pos_emb_max_len: 1024
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
batch_size: 1
valid_batch_size: 1
batch_type: tokens
accum_count: [1, 3, 6, 12, 18, 30]
accum_steps: [0, 5000, 30000, 50000, 75000, 100000]
share_decoder_embeddings: true
batch_size_multiple: 1

seed: 42
share_vocab: true


model_dtype: "fp32"
optim: "adam"
learning_rate: 1
warmup_steps: 16000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 1
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

tensorboard: true
tensorboard_log_dir: /nas-labs/LM/valentin_work/OpenNMT-py/data_run/huggingface/run/tensorboard
